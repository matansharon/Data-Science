{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvmlInit()\n",
    "deviceCount = nvmlDeviceGetCount()\n",
    "for i in range(deviceCount):\n",
    "    handle = nvmlDeviceGetHandleByIndex(i)\n",
    "    print(\"Device\", i, \":\", nvmlDeviceGetName(handle))\n",
    "nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Built with CUDA: \", tf.test.is_built_with_cuda())\n",
    "print(\"Built with GPU Support: \", tf.test.is_built_with_gpu_support())\n",
    "import torch\n",
    "\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"CUDA version: \", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, cuda\n",
    "import numpy as np\n",
    "# to measure exec time\n",
    "from timeit import default_timer as timer   \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "# normal function to run on cpu\n",
    "@jit(target_backend='cpu')\n",
    "def func(a,n,size=5000):                                \n",
    "    for i in range(n):\n",
    "        a = tf.random.uniform(shape=[size, size])     \n",
    "  \n",
    "# function optimized to run on gpu \n",
    "@jit(target_backend='cuda')                         \n",
    "def func2(a,n,size=5000):\n",
    "    for i in range(n):\n",
    "        a = tf.random.uniform(shape=[size, size])\n",
    "if __name__==\"__main__\":\n",
    "    n = 100                       \n",
    "    a = np.ones(n, dtype = np.float64)\n",
    "      \n",
    "    start = timer()\n",
    "    func(a,n)\n",
    "    print(\"without GPU:\", timer()-start)    \n",
    "      \n",
    "    start = timer()\n",
    "    func2(a,n)\n",
    "    print(\"with GPU:\", timer()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device_name = tf.test.gpu_device_name() if tf.test.is_gpu_available() else \"/device:CPU:0\"\n",
    "print(device_name)\n",
    "# Number of rounds for the test\n",
    "rounds = 100\n",
    "size = 5000\n",
    "\n",
    "# Create some large matrix\n",
    "a = tf.random.uniform(shape=[size, size])\n",
    "\n",
    "# Run the test\n",
    "start = time.time()\n",
    "\n",
    "with tf.device(device_name):\n",
    "    for _ in range(rounds):\n",
    "        tf.linalg.matmul(a, a)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken to perform {rounds} rounds of matrix multiplication of size {size}x{size} : {end - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f07222ca73445cb29252ebc665a27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/963 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726060b386c4444d8a48c1e9afb32d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69af2698bbdb40a2960e0fdbb8ccde7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ed487d997e4b1882470c409fbd10c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83929caf24924ef4b7941bbfd5dce713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1eb854b7cb4730bbd489425db2db13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da498e964ef743398c8534d440f75746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s><s>[INST] Tell me about AI [/INST]\n",
      "AI, or artificial intelligence, is the development of computer systems that are able to perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI technology is constantly evolving and is being used in a wide range of applications, from virtual assistants and chatbots to self-driving cars and medical diagnosis. Some of the most common types of AI include narrow or weak AI, which is designed to perform a specific task, and general or strong AI, which is designed to be able to learn and perform any intellectual task that a human can.</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path='/teamspace/studios/this_studio/mistral-7b-gptq'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# print(\"*** Pipeline:\")\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.95,\n",
    "#     top_k=40,\n",
    "#     repetition_penalty=1.1\n",
    "# )\n",
    "\n",
    "# print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(input_text):\n",
    "    prompt_template=f'''<s>[INST] {input_text} [/INST]\n",
    "    '''\n",
    "    input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def get_pages(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    res=[]\n",
    "    for page in reader.pages:\n",
    "        res.append(page.extract_text())\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table 1: Elo ratings for a competition between\\nmodels, averaged for 10,000 random initial order-\\nings. The winner of a match is determined by\\nGPT-4 which declares which response is better for\\na given prompt of the the Vicuna benchmark. 95%\\nconfidence intervals are shown ( ±). After GPT-\\n4, Guanaco 33B and 65B win the most matches,\\nwhile Guanaco 13B scores better than Bard.\\nModel Size Elo\\nGPT-4 - 1348 ±1\\nGuanaco 65B 41 GB 1022 ±1\\nGuanaco 33B 21 GB 992 ±1\\nVicuna 13B 26 GB 974 ±1\\nChatGPT - 966 ±1\\nGuanaco 13B 10 GB 916 ±1\\nBard - 902 ±1\\nGuanaco 7B 6 GB 879 ±1that are tuned by backpropagating gradients through\\nthe quantized weights.\\nQLORAreduces the average memory requirements\\nof finetuning a 65B parameter model from >780GB\\nof GPU memory to <48GB without degrading the\\nruntime or predictive performance compared to a 16-\\nbit fully finetuned baseline. This marks a significant\\nshift in accessibility of LLM finetuning: now the\\nlargest publicly available models to date finetunable\\non a single GPU. Using QLORA, we train the Gua-\\nnaco family of models, with the second best model\\nreaching 97.8% of the performance level of ChatGPT\\non the Vicuna [ 10] benchmark, while being trainable\\nin less than 12 hours on a single consumer GPU;\\nusing a single professional GPU over 24 hours we\\nachieve 99.3% with our largest model, essentially\\nclosing the gap to ChatGPT on the Vicuna bench-\\nmark. When deployed, our smallest Guanaco model\\n(7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than\\n20 percentage points on the Vicuna benchmark (Table 6).\\nQLORAintroduces multiple innovations designed to reduce memory use without sacrificing per-\\nformance: (1) 4-bit NormalFloat , an information theoretically optimal quantization data type for\\nnormally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.\\n(2)Double Quantization , a method that quantizes the quantization constants, saving an average\\nof about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3) Paged Optimizers ,\\nusing NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when\\nprocessing a mini-batch with a long sequence length. We combine these contributions into a better\\ntuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of\\nthe accuracy tradeoffs seen in prior work.\\nQLORA’s efficiency enables us to perform an in-depth study of instruction finetuning and chatbot\\nperformance on model scales that would be impossible using regular finetuning due to memory\\noverhead. Therefore, we train more than 1,000 models across several instruction tuning datasets,\\nmodel architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA\\nrecovers 16-bit performance (§4) and training a state-of-the-art chatbot, Guanaco , (§5), we also\\nanalyze trends in the trained models. First, we find that data quality is far more important than\\ndataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,\\nsubsampled) on chatbot performance, even when both are meant to support instruction following\\ngeneralization. Second, we show that strong Massive Multitask Language Understanding (MMLU)\\nbenchmark performance does not imply strong Vicuna chatbot benchmark performance and vice\\nversa—in other words, dataset suitability matters more than size for a given task.\\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human\\nraters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\\nagainst each other in matches to produce the best response for a given prompt. The winner of a\\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into\\nElo scores [ 16,17] which determine the ranking of chatbot performance. We find that GPT-4 and\\nhuman evaluations largely agree on the rank of model performance in the tournaments, but we also\\nfind there are instances of strong disagreement. As such, we highlight that model-based evaluation\\nwhile providing a cheap alternative to human-annotation also has its uncertainties.\\nWe augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy-\\nsis highlights success and failure cases that were not captured by the quantitative benchmarks.\\nWe release all model generations with human and GPT-4 annotations to facilitate further study. We\\nopen-source our codebase and CUDA kernels and integrate our methods into the Hugging Face\\ntransformers stack [ 64], making them easily accessible to all. We release a collection of adapters\\nfor 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32\\ndifferent open sourced, finetuned models.\\n2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "pages=get_pages(\"/teamspace/studios/this_studio/data_science/some tests/qlora.pdf\")\n",
    "pages[1]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Table',\n",
       " '1',\n",
       " ':',\n",
       " 'Elo',\n",
       " 'ratings',\n",
       " 'for',\n",
       " 'a',\n",
       " 'competition',\n",
       " 'between',\n",
       " 'models',\n",
       " ',',\n",
       " 'averaged',\n",
       " 'for',\n",
       " '10,000',\n",
       " 'random',\n",
       " 'initial',\n",
       " 'order-',\n",
       " 'ings',\n",
       " '.',\n",
       " 'The',\n",
       " 'winner',\n",
       " 'of',\n",
       " 'a',\n",
       " 'match',\n",
       " 'is',\n",
       " 'determined',\n",
       " 'by',\n",
       " 'GPT-4',\n",
       " 'which',\n",
       " 'declares',\n",
       " 'which',\n",
       " 'response',\n",
       " 'is',\n",
       " 'better',\n",
       " 'for',\n",
       " 'a',\n",
       " 'given',\n",
       " 'prompt',\n",
       " 'of',\n",
       " 'the',\n",
       " 'the',\n",
       " 'Vicuna',\n",
       " 'benchmark',\n",
       " '.',\n",
       " '95',\n",
       " '%',\n",
       " 'confidence',\n",
       " 'intervals',\n",
       " 'are',\n",
       " 'shown',\n",
       " '(',\n",
       " '±',\n",
       " ')',\n",
       " '.',\n",
       " 'After',\n",
       " 'GPT-',\n",
       " '4',\n",
       " ',',\n",
       " 'Guanaco',\n",
       " '33B',\n",
       " 'and',\n",
       " '65B',\n",
       " 'win',\n",
       " 'the',\n",
       " 'most',\n",
       " 'matches',\n",
       " ',',\n",
       " 'while',\n",
       " 'Guanaco',\n",
       " '13B',\n",
       " 'scores',\n",
       " 'better',\n",
       " 'than',\n",
       " 'Bard',\n",
       " '.',\n",
       " 'Model',\n",
       " 'Size',\n",
       " 'Elo',\n",
       " 'GPT-4',\n",
       " '-',\n",
       " '1348',\n",
       " '±1',\n",
       " 'Guanaco',\n",
       " '65B',\n",
       " '41',\n",
       " 'GB',\n",
       " '1022',\n",
       " '±1',\n",
       " 'Guanaco',\n",
       " '33B',\n",
       " '21',\n",
       " 'GB',\n",
       " '992',\n",
       " '±1',\n",
       " 'Vicuna',\n",
       " '13B',\n",
       " '26',\n",
       " 'GB',\n",
       " '974',\n",
       " '±1',\n",
       " 'ChatGPT',\n",
       " '-',\n",
       " '966',\n",
       " '±1',\n",
       " 'Guanaco',\n",
       " '13B',\n",
       " '10',\n",
       " 'GB',\n",
       " '916',\n",
       " '±1',\n",
       " 'Bard',\n",
       " '-',\n",
       " '902',\n",
       " '±1',\n",
       " 'Guanaco',\n",
       " '7B',\n",
       " '6',\n",
       " 'GB',\n",
       " '879',\n",
       " '±1that',\n",
       " 'are',\n",
       " 'tuned',\n",
       " 'by',\n",
       " 'backpropagating',\n",
       " 'gradients',\n",
       " 'through',\n",
       " 'the',\n",
       " 'quantized',\n",
       " 'weights',\n",
       " '.',\n",
       " 'QLORAreduces',\n",
       " 'the',\n",
       " 'average',\n",
       " 'memory',\n",
       " 'requirements',\n",
       " 'of',\n",
       " 'finetuning',\n",
       " 'a',\n",
       " '65B',\n",
       " 'parameter',\n",
       " 'model',\n",
       " 'from',\n",
       " '>',\n",
       " '780GB',\n",
       " 'of',\n",
       " 'GPU',\n",
       " 'memory',\n",
       " 'to',\n",
       " '<',\n",
       " '48GB',\n",
       " 'without',\n",
       " 'degrading',\n",
       " 'the',\n",
       " 'runtime',\n",
       " 'or',\n",
       " 'predictive',\n",
       " 'performance',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'a',\n",
       " '16-',\n",
       " 'bit',\n",
       " 'fully',\n",
       " 'finetuned',\n",
       " 'baseline',\n",
       " '.',\n",
       " 'This',\n",
       " 'marks',\n",
       " 'a',\n",
       " 'significant',\n",
       " 'shift',\n",
       " 'in',\n",
       " 'accessibility',\n",
       " 'of',\n",
       " 'LLM',\n",
       " 'finetuning',\n",
       " ':',\n",
       " 'now',\n",
       " 'the',\n",
       " 'largest',\n",
       " 'publicly',\n",
       " 'available',\n",
       " 'models',\n",
       " 'to',\n",
       " 'date',\n",
       " 'finetunable',\n",
       " 'on',\n",
       " 'a',\n",
       " 'single',\n",
       " 'GPU',\n",
       " '.',\n",
       " 'Using',\n",
       " 'QLORA',\n",
       " ',',\n",
       " 'we',\n",
       " 'train',\n",
       " 'the',\n",
       " 'Gua-',\n",
       " 'naco',\n",
       " 'family',\n",
       " 'of',\n",
       " 'models',\n",
       " ',',\n",
       " 'with',\n",
       " 'the',\n",
       " 'second',\n",
       " 'best',\n",
       " 'model',\n",
       " 'reaching',\n",
       " '97.8',\n",
       " '%',\n",
       " 'of',\n",
       " 'the',\n",
       " 'performance',\n",
       " 'level',\n",
       " 'of',\n",
       " 'ChatGPT',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Vicuna',\n",
       " '[',\n",
       " '10',\n",
       " ']',\n",
       " 'benchmark',\n",
       " ',',\n",
       " 'while',\n",
       " 'being',\n",
       " 'trainable',\n",
       " 'in',\n",
       " 'less',\n",
       " 'than',\n",
       " '12',\n",
       " 'hours',\n",
       " 'on',\n",
       " 'a',\n",
       " 'single',\n",
       " 'consumer',\n",
       " 'GPU',\n",
       " ';',\n",
       " 'using',\n",
       " 'a',\n",
       " 'single',\n",
       " 'professional',\n",
       " 'GPU',\n",
       " 'over',\n",
       " '24',\n",
       " 'hours',\n",
       " 'we',\n",
       " 'achieve',\n",
       " '99.3',\n",
       " '%',\n",
       " 'with',\n",
       " 'our',\n",
       " 'largest',\n",
       " 'model',\n",
       " ',',\n",
       " 'essentially',\n",
       " 'closing',\n",
       " 'the',\n",
       " 'gap',\n",
       " 'to',\n",
       " 'ChatGPT',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Vicuna',\n",
       " 'bench-',\n",
       " 'mark',\n",
       " '.',\n",
       " 'When',\n",
       " 'deployed',\n",
       " ',',\n",
       " 'our',\n",
       " 'smallest',\n",
       " 'Guanaco',\n",
       " 'model',\n",
       " '(',\n",
       " '7B',\n",
       " 'parameters',\n",
       " ')',\n",
       " 'requires',\n",
       " 'just',\n",
       " '5',\n",
       " 'GB',\n",
       " 'of',\n",
       " 'memory',\n",
       " 'and',\n",
       " 'outperforms',\n",
       " 'a',\n",
       " '26',\n",
       " 'GB',\n",
       " 'Alpaca',\n",
       " 'model',\n",
       " 'by',\n",
       " 'more',\n",
       " 'than',\n",
       " '20',\n",
       " 'percentage',\n",
       " 'points',\n",
       " 'on',\n",
       " 'the',\n",
       " 'Vicuna',\n",
       " 'benchmark',\n",
       " '(',\n",
       " 'Table',\n",
       " '6',\n",
       " ')',\n",
       " '.',\n",
       " 'QLORAintroduces',\n",
       " 'multiple',\n",
       " 'innovations',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'reduce',\n",
       " 'memory',\n",
       " 'use',\n",
       " 'without',\n",
       " 'sacrificing',\n",
       " 'per-',\n",
       " 'formance',\n",
       " ':',\n",
       " '(',\n",
       " '1',\n",
       " ')',\n",
       " '4-bit',\n",
       " 'NormalFloat',\n",
       " ',',\n",
       " 'an',\n",
       " 'information',\n",
       " 'theoretically',\n",
       " 'optimal',\n",
       " 'quantization',\n",
       " 'data',\n",
       " 'type',\n",
       " 'for',\n",
       " 'normally',\n",
       " 'distributed',\n",
       " 'data',\n",
       " 'that',\n",
       " 'yields',\n",
       " 'better',\n",
       " 'empirical',\n",
       " 'results',\n",
       " 'than',\n",
       " '4-bit',\n",
       " 'Integers',\n",
       " 'and',\n",
       " '4-bit',\n",
       " 'Floats',\n",
       " '.',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " 'Double',\n",
       " 'Quantization',\n",
       " ',',\n",
       " 'a',\n",
       " 'method',\n",
       " 'that',\n",
       " 'quantizes',\n",
       " 'the',\n",
       " 'quantization',\n",
       " 'constants',\n",
       " ',',\n",
       " 'saving',\n",
       " 'an',\n",
       " 'average',\n",
       " 'of',\n",
       " 'about',\n",
       " '0.37',\n",
       " 'bits',\n",
       " 'per',\n",
       " 'parameter',\n",
       " '(',\n",
       " 'approximately',\n",
       " '3',\n",
       " 'GB',\n",
       " 'for',\n",
       " 'a',\n",
       " '65B',\n",
       " 'model',\n",
       " ')',\n",
       " '.',\n",
       " '(',\n",
       " '3',\n",
       " ')',\n",
       " 'Paged',\n",
       " 'Optimizers',\n",
       " ',',\n",
       " 'using',\n",
       " 'NVIDIA',\n",
       " 'unified',\n",
       " 'memory',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'the',\n",
       " 'gradient',\n",
       " 'checkpointing',\n",
       " 'memory',\n",
       " 'spikes',\n",
       " 'that',\n",
       " 'occur',\n",
       " 'when',\n",
       " 'processing',\n",
       " 'a',\n",
       " 'mini-batch',\n",
       " 'with',\n",
       " 'a',\n",
       " 'long',\n",
       " 'sequence',\n",
       " 'length',\n",
       " '.',\n",
       " 'We',\n",
       " 'combine',\n",
       " 'these',\n",
       " 'contributions',\n",
       " 'into',\n",
       " 'a',\n",
       " 'better',\n",
       " 'tuned',\n",
       " 'LoRA',\n",
       " 'approach',\n",
       " 'that',\n",
       " 'includes',\n",
       " 'adapters',\n",
       " 'at',\n",
       " 'every',\n",
       " 'network',\n",
       " 'layer',\n",
       " 'and',\n",
       " 'thereby',\n",
       " 'avoids',\n",
       " 'almost',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'accuracy',\n",
       " 'tradeoffs',\n",
       " 'seen',\n",
       " 'in',\n",
       " 'prior',\n",
       " 'work',\n",
       " '.',\n",
       " 'QLORA',\n",
       " '’',\n",
       " 's',\n",
       " 'efficiency',\n",
       " 'enables',\n",
       " 'us',\n",
       " 'to',\n",
       " 'perform',\n",
       " 'an',\n",
       " 'in-depth',\n",
       " 'study',\n",
       " 'of',\n",
       " 'instruction',\n",
       " 'finetuning',\n",
       " 'and',\n",
       " 'chatbot',\n",
       " 'performance',\n",
       " 'on',\n",
       " 'model',\n",
       " 'scales',\n",
       " 'that',\n",
       " 'would',\n",
       " 'be',\n",
       " 'impossible',\n",
       " 'using',\n",
       " 'regular',\n",
       " 'finetuning',\n",
       " 'due',\n",
       " 'to',\n",
       " 'memory',\n",
       " 'overhead',\n",
       " '.',\n",
       " 'Therefore',\n",
       " ',',\n",
       " 'we',\n",
       " 'train',\n",
       " 'more',\n",
       " 'than',\n",
       " '1,000',\n",
       " 'models',\n",
       " 'across',\n",
       " 'several',\n",
       " 'instruction',\n",
       " 'tuning',\n",
       " 'datasets',\n",
       " ',',\n",
       " 'model',\n",
       " 'architectures',\n",
       " ',',\n",
       " 'and',\n",
       " 'sizes',\n",
       " 'between',\n",
       " '80M',\n",
       " 'to',\n",
       " '65B',\n",
       " 'parameters',\n",
       " '.',\n",
       " 'In',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'showing',\n",
       " 'that',\n",
       " 'QLORA',\n",
       " 'recovers',\n",
       " '16-bit',\n",
       " 'performance',\n",
       " '(',\n",
       " '§4',\n",
       " ')',\n",
       " 'and',\n",
       " 'training',\n",
       " 'a',\n",
       " 'state-of-the-art',\n",
       " 'chatbot',\n",
       " ',',\n",
       " 'Guanaco',\n",
       " ',',\n",
       " '(',\n",
       " '§5',\n",
       " ')',\n",
       " ',',\n",
       " 'we',\n",
       " 'also',\n",
       " 'analyze',\n",
       " 'trends',\n",
       " 'in',\n",
       " 'the',\n",
       " 'trained',\n",
       " 'models',\n",
       " '.',\n",
       " 'First',\n",
       " ',',\n",
       " 'we',\n",
       " 'find',\n",
       " 'that',\n",
       " 'data',\n",
       " 'quality',\n",
       " 'is',\n",
       " 'far',\n",
       " 'more',\n",
       " 'important',\n",
       " 'than',\n",
       " 'dataset',\n",
       " 'size',\n",
       " ',',\n",
       " 'e.g.',\n",
       " ',',\n",
       " 'a',\n",
       " '9k',\n",
       " 'sample',\n",
       " 'dataset',\n",
       " '(',\n",
       " 'OASST1',\n",
       " ')',\n",
       " 'outperformed',\n",
       " 'a',\n",
       " '450k',\n",
       " 'sample',\n",
       " 'dataset',\n",
       " '(',\n",
       " 'FLAN',\n",
       " 'v2',\n",
       " ',',\n",
       " 'subsampled',\n",
       " ')',\n",
       " 'on',\n",
       " 'chatbot',\n",
       " 'performance',\n",
       " ',',\n",
       " 'even',\n",
       " 'when',\n",
       " 'both',\n",
       " 'are',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'support',\n",
       " 'instruction',\n",
       " 'following',\n",
       " 'generalization',\n",
       " '.',\n",
       " 'Second',\n",
       " ',',\n",
       " 'we',\n",
       " 'show',\n",
       " 'that',\n",
       " 'strong',\n",
       " 'Massive',\n",
       " 'Multitask',\n",
       " 'Language',\n",
       " 'Understanding',\n",
       " '(',\n",
       " 'MMLU',\n",
       " ')',\n",
       " 'benchmark',\n",
       " 'performance',\n",
       " 'does',\n",
       " 'not',\n",
       " 'imply',\n",
       " 'strong',\n",
       " 'Vicuna',\n",
       " 'chatbot',\n",
       " 'benchmark',\n",
       " 'performance',\n",
       " 'and',\n",
       " 'vice',\n",
       " 'versa—in',\n",
       " 'other',\n",
       " 'words',\n",
       " ',',\n",
       " 'dataset',\n",
       " 'suitability',\n",
       " 'matters',\n",
       " 'more',\n",
       " 'than',\n",
       " 'size',\n",
       " 'for',\n",
       " 'a',\n",
       " 'given',\n",
       " 'task',\n",
       " '.',\n",
       " 'Furthermore',\n",
       " ',',\n",
       " 'we',\n",
       " 'also',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'extensive',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'chatbot',\n",
       " 'performance',\n",
       " 'that',\n",
       " 'uses',\n",
       " 'both',\n",
       " 'human',\n",
       " 'raters',\n",
       " 'and',\n",
       " 'GPT-4',\n",
       " 'for',\n",
       " 'evaluation',\n",
       " '.',\n",
       " 'We',\n",
       " 'use',\n",
       " 'tournament-style',\n",
       " 'benchmarking',\n",
       " 'where',\n",
       " 'models',\n",
       " 'compete',\n",
       " 'against',\n",
       " 'each',\n",
       " 'other',\n",
       " 'in',\n",
       " 'matches',\n",
       " 'to',\n",
       " 'produce',\n",
       " 'the',\n",
       " 'best',\n",
       " 'response',\n",
       " 'for',\n",
       " 'a',\n",
       " 'given',\n",
       " 'prompt',\n",
       " '.',\n",
       " 'The',\n",
       " 'winner',\n",
       " 'of',\n",
       " 'a',\n",
       " 'match',\n",
       " 'is',\n",
       " 'judged',\n",
       " 'by',\n",
       " 'either',\n",
       " 'GPT-4',\n",
       " 'or',\n",
       " 'human',\n",
       " 'annotators',\n",
       " '.',\n",
       " 'The',\n",
       " 'tournament',\n",
       " 'results',\n",
       " 'are',\n",
       " 'aggregated',\n",
       " 'into',\n",
       " 'Elo',\n",
       " 'scores',\n",
       " '[',\n",
       " '16,17',\n",
       " ']',\n",
       " 'which',\n",
       " 'determine',\n",
       " 'the',\n",
       " 'ranking',\n",
       " 'of',\n",
       " 'chatbot',\n",
       " 'performance',\n",
       " '.',\n",
       " 'We',\n",
       " 'find',\n",
       " 'that',\n",
       " 'GPT-4',\n",
       " 'and',\n",
       " 'human',\n",
       " 'evaluations',\n",
       " 'largely',\n",
       " 'agree',\n",
       " 'on',\n",
       " 'the',\n",
       " 'rank',\n",
       " 'of',\n",
       " 'model',\n",
       " 'performance',\n",
       " 'in',\n",
       " 'the',\n",
       " 'tournaments',\n",
       " ',',\n",
       " 'but',\n",
       " 'we',\n",
       " 'also',\n",
       " 'find',\n",
       " 'there',\n",
       " 'are',\n",
       " 'instances',\n",
       " 'of',\n",
       " 'strong',\n",
       " 'disagreement',\n",
       " '.',\n",
       " 'As',\n",
       " 'such',\n",
       " ',',\n",
       " 'we',\n",
       " 'highlight',\n",
       " 'that',\n",
       " 'model-based',\n",
       " 'evaluation',\n",
       " 'while',\n",
       " 'providing',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'alternative',\n",
       " 'to',\n",
       " 'human-annotation',\n",
       " 'also',\n",
       " 'has',\n",
       " 'its',\n",
       " 'uncertainties',\n",
       " '.',\n",
       " 'We',\n",
       " 'augment',\n",
       " 'our',\n",
       " 'chatbot',\n",
       " 'benchmark',\n",
       " 'results',\n",
       " 'with',\n",
       " 'a',\n",
       " 'qualitative',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'Guanaco',\n",
       " 'models',\n",
       " '.',\n",
       " 'Our',\n",
       " 'analy-',\n",
       " 'sis',\n",
       " 'highlights',\n",
       " 'success',\n",
       " 'and',\n",
       " 'failure',\n",
       " 'cases',\n",
       " 'that',\n",
       " 'were',\n",
       " 'not',\n",
       " 'captured',\n",
       " 'by',\n",
       " 'the',\n",
       " 'quantitative',\n",
       " 'benchmarks',\n",
       " '.',\n",
       " 'We',\n",
       " 'release',\n",
       " 'all',\n",
       " 'model',\n",
       " 'generations',\n",
       " 'with',\n",
       " 'human',\n",
       " 'and',\n",
       " 'GPT-4',\n",
       " 'annotations',\n",
       " 'to',\n",
       " 'facilitate',\n",
       " 'further',\n",
       " 'study',\n",
       " '.',\n",
       " 'We',\n",
       " 'open-source',\n",
       " 'our',\n",
       " 'codebase',\n",
       " 'and',\n",
       " 'CUDA',\n",
       " 'kernels',\n",
       " 'and',\n",
       " 'integrate',\n",
       " 'our',\n",
       " 'methods',\n",
       " 'into',\n",
       " 'the',\n",
       " 'Hugging',\n",
       " 'Face',\n",
       " 'transformers',\n",
       " 'stack',\n",
       " '[',\n",
       " '64',\n",
       " ']',\n",
       " ',',\n",
       " 'making',\n",
       " 'them',\n",
       " 'easily',\n",
       " 'accessible',\n",
       " 'to',\n",
       " 'all',\n",
       " '.',\n",
       " 'We',\n",
       " 'release',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'adapters',\n",
       " 'for',\n",
       " '7/13/33/65B',\n",
       " 'size',\n",
       " 'models',\n",
       " ',',\n",
       " 'trained',\n",
       " 'on',\n",
       " '8',\n",
       " 'different',\n",
       " 'instruction',\n",
       " 'following',\n",
       " 'datasets',\n",
       " ',',\n",
       " 'for',\n",
       " 'a',\n",
       " 'total',\n",
       " 'of',\n",
       " '32',\n",
       " 'different',\n",
       " 'open',\n",
       " 'sourced',\n",
       " ',',\n",
       " 'finetuned',\n",
       " 'models',\n",
       " '.',\n",
       " '2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
