{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvmlInit()\n",
    "deviceCount = nvmlDeviceGetCount()\n",
    "for i in range(deviceCount):\n",
    "    handle = nvmlDeviceGetHandleByIndex(i)\n",
    "    print(\"Device\", i, \":\", nvmlDeviceGetName(handle))\n",
    "nvmlShutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Built with CUDA: \", tf.test.is_built_with_cuda())\n",
    "print(\"Built with GPU Support: \", tf.test.is_built_with_gpu_support())\n",
    "import torch\n",
    "\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"CUDA version: \", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit, cuda\n",
    "import numpy as np\n",
    "# to measure exec time\n",
    "from timeit import default_timer as timer   \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "# normal function to run on cpu\n",
    "@jit(target_backend='cpu')\n",
    "def func(a,n,size=5000):                                \n",
    "    for i in range(n):\n",
    "        a = tf.random.uniform(shape=[size, size])     \n",
    "  \n",
    "# function optimized to run on gpu \n",
    "@jit(target_backend='cuda')                         \n",
    "def func2(a,n,size=5000):\n",
    "    for i in range(n):\n",
    "        a = tf.random.uniform(shape=[size, size])\n",
    "if __name__==\"__main__\":\n",
    "    n = 100                       \n",
    "    a = np.ones(n, dtype = np.float64)\n",
    "      \n",
    "    start = timer()\n",
    "    func(a,n)\n",
    "    print(\"without GPU:\", timer()-start)    \n",
    "      \n",
    "    start = timer()\n",
    "    func2(a,n)\n",
    "    print(\"with GPU:\", timer()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device_name = tf.test.gpu_device_name() if tf.test.is_gpu_available() else \"/device:CPU:0\"\n",
    "print(device_name)\n",
    "# Number of rounds for the test\n",
    "rounds = 100\n",
    "size = 5000\n",
    "\n",
    "# Create some large matrix\n",
    "a = tf.random.uniform(shape=[size, size])\n",
    "\n",
    "# Run the test\n",
    "start = time.time()\n",
    "\n",
    "with tf.device(device_name):\n",
    "    for _ in range(rounds):\n",
    "        tf.linalg.matmul(a, a)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time taken to perform {rounds} rounds of matrix multiplication of size {size}x{size} : {end - start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f07222ca73445cb29252ebc665a27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/963 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726060b386c4444d8a48c1e9afb32d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69af2698bbdb40a2960e0fdbb8ccde7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ed487d997e4b1882470c409fbd10c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83929caf24924ef4b7941bbfd5dce713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1eb854b7cb4730bbd489425db2db13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da498e964ef743398c8534d440f75746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s><s>[INST] Tell me about AI [/INST]\n",
      "AI, or artificial intelligence, is the development of computer systems that are able to perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI technology is constantly evolving and is being used in a wide range of applications, from virtual assistants and chatbots to self-driving cars and medical diagnosis. Some of the most common types of AI include narrow or weak AI, which is designed to perform a specific task, and general or strong AI, which is designed to be able to learn and perform any intellectual task that a human can.</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path='/teamspace/studios/this_studio/mistral-7b-gptq'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# print(\"*** Pipeline:\")\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.95,\n",
    "#     top_k=40,\n",
    "#     repetition_penalty=1.1\n",
    "# )\n",
    "\n",
    "# print(pipe(prompt_template)[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(input_text):\n",
    "    prompt_template=f'''<s>[INST] {input_text} [/INST]\n",
    "    '''\n",
    "    input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "    return tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def get_pages(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    res=[]\n",
    "    for page in reader.pages:\n",
    "        res.append(page.extract_text())\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "pages=get_pages(\"/teamspace/studios/this_studio/data_science/some tests/qlora.pdf\")\n",
    "pages[1]\n",
    "len(pages)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QL ORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORAintroduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double\\nQuantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\\nfinetuning on a small high-quality dataset leads to state-of-the-art results, even\\nwhen using smaller models than the previous SoTA. We provide a detailed analysis\\nof chatbot performance based on both human and GPT-4 evaluations showing that\\nGPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\\nthermore, we find that current chatbot benchmarks are not trustworthy to accurately\\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\\nwhere Guanaco fails compared to ChatGPT. We release all of our models and code,\\nincluding CUDA kernels for 4-bit training.2\\n1 Introduction\\nFinetuning large language models (LLMs) is a highly effective way to improve their performance,\\n[40,62,43,61,59,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,\\nfinetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\\nparameter model [ 57] requires more than 780 GB of GPU memory. While recent quantization\\nmethods can reduce the memory footprint of LLMs [ 14,13,18,66], such techniques only work for\\ninference and break down during training [65].\\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\\nperformance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]\\n∗Equal contribution.\\n2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\\nPreprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023Table 1: Elo ratings for a competition between\\nmodels, averaged for 10,000 random initial order-\\nings. The winner of a match is determined by\\nGPT-4 which declares which response is better for\\na given prompt of the the Vicuna benchmark. 95%\\nconfidence intervals are shown ( ±). After GPT-\\n4, Guanaco 33B and 65B win the most matches,\\nwhile Guanaco 13B scores better than Bard.\\nModel Size Elo\\nGPT-4 - 1348 ±1\\nGuanaco 65B 41 GB 1022 ±1\\nGuanaco 33B 21 GB 992 ±1\\nVicuna 13B 26 GB 974 ±1\\nChatGPT - 966 ±1\\nGuanaco 13B 10 GB 916 ±1\\nBard - 902 ±1\\nGuanaco 7B 6 GB 879 ±1that are tuned by backpropagating gradients through\\nthe quantized weights.\\nQLORAreduces the average memory requirements\\nof finetuning a 65B parameter model from >780GB\\nof GPU memory to <48GB without degrading the\\nruntime or predictive performance compared to a 16-\\nbit fully finetuned baseline. This marks a significant\\nshift in accessibility of LLM finetuning: now the\\nlargest publicly available models to date finetunable\\non a single GPU. Using QLORA, we train the Gua-\\nnaco family of models, with the second best model\\nreaching 97.8% of the performance level of ChatGPT\\non the Vicuna [ 10] benchmark, while being trainable\\nin less than 12 hours on a single consumer GPU;\\nusing a single professional GPU over 24 hours we\\nachieve 99.3% with our largest model, essentially\\nclosing the gap to ChatGPT on the Vicuna bench-\\nmark. When deployed, our smallest Guanaco model\\n(7B parameters) requires just 5 GB of memory and outperforms a 26 GB Alpaca model by more than\\n20 percentage points on the Vicuna benchmark (Table 6).\\nQLORAintroduces multiple innovations designed to reduce memory use without sacrificing per-\\nformance: (1) 4-bit NormalFloat , an information theoretically optimal quantization data type for\\nnormally distributed data that yields better empirical results than 4-bit Integers and 4-bit Floats.\\n(2)Double Quantization , a method that quantizes the quantization constants, saving an average\\nof about 0.37 bits per parameter (approximately 3 GB for a 65B model). (3) Paged Optimizers ,\\nusing NVIDIA unified memory to avoid the gradient checkpointing memory spikes that occur when\\nprocessing a mini-batch with a long sequence length. We combine these contributions into a better\\ntuned LoRA approach that includes adapters at every network layer and thereby avoids almost all of\\nthe accuracy tradeoffs seen in prior work.\\nQLORA’s efficiency enables us to perform an in-depth study of instruction finetuning and chatbot\\nperformance on model scales that would be impossible using regular finetuning due to memory\\noverhead. Therefore, we train more than 1,000 models across several instruction tuning datasets,\\nmodel architectures, and sizes between 80M to 65B parameters. In addition to showing that QLORA\\nrecovers 16-bit performance (§4) and training a state-of-the-art chatbot, Guanaco , (§5), we also\\nanalyze trends in the trained models. First, we find that data quality is far more important than\\ndataset size, e.g., a 9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2,\\nsubsampled) on chatbot performance, even when both are meant to support instruction following\\ngeneralization. Second, we show that strong Massive Multitask Language Understanding (MMLU)\\nbenchmark performance does not imply strong Vicuna chatbot benchmark performance and vice\\nversa—in other words, dataset suitability matters more than size for a given task.\\nFurthermore, we also provide a extensive analysis of chatbot performance that uses both human\\nraters and GPT-4 for evaluation. We use tournament-style benchmarking where models compete\\nagainst each other in matches to produce the best response for a given prompt. The winner of a\\nmatch is judged by either GPT-4 or human annotators. The tournament results are aggregated into\\nElo scores [ 16,17] which determine the ranking of chatbot performance. We find that GPT-4 and\\nhuman evaluations largely agree on the rank of model performance in the tournaments, but we also\\nfind there are instances of strong disagreement. As such, we highlight that model-based evaluation\\nwhile providing a cheap alternative to human-annotation also has its uncertainties.\\nWe augment our chatbot benchmark results with a qualitative analysis of Guanaco models. Our analy-\\nsis highlights success and failure cases that were not captured by the quantitative benchmarks.\\nWe release all model generations with human and GPT-4 annotations to facilitate further study. We\\nopen-source our codebase and CUDA kernels and integrate our methods into the Hugging Face\\ntransformers stack [ 64], making them easily accessible to all. We release a collection of adapters\\nfor 7/13/33/65B size models, trained on 8 different instruction following datasets, for a total of 32\\ndifferent open sourced, finetuned models.\\n2Figure 1: Different finetuning methods and their memory requirements. QLORAimproves over LoRA by\\nquantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes.\\n2 Background\\nBlock-wise k-bit Quantization Quantization is the process of discretizing an input from a rep-\\nresentation that holds more information to a representation with less information. It often means\\ntaking a data type with more bits and converting it to fewer bits, for example from 32-bit floats to\\n8-bit Integers. To ensure that the entire range of the low-bit data type is used, the input data type is\\ncommonly rescaled into the target data type range through normalization by the absolute maximum\\nof the input elements, which are usually structured as a tensor. For example, quantizing a 32-bit\\nFloating Point (FP32) tensor into a Int8 tensor with range [−127,127]:\\nXInt8=round\\x12127\\nabsmax (XFP32)XFP32\\x13\\n=round (cFP32·XFP32), (1)\\nwhere cis the quantization constant orquantization scale . Dequantization is the inverse:\\ndequant (cFP32,XInt8) =XInt8\\ncFP32=XFP32(2)\\nThe problem with this approach is that if a large magnitude value (i.e., an outlier) occurs in the input\\ntensor, then the quantization bins—certain bit combinations—are not utilized well with few or no\\nnumbers quantized in some bins. To prevent the outlier issue, a common approach is to chunk the\\ninput tensor into blocks that are independently quantized, each with their own quantization constant c.\\nThis can be formalized as follows: We chunk the input tensor X∈Rb×hintoncontiguous blocks of\\nsizeBby flattening the input tensor and slicing the linear segment into n= (b×h)/Bblocks. We\\nquantize these blocks independently with Equation 1 to create a quantized tensor and nquantization\\nconstants ci.\\nLow-rank Adapters Low-rank Adapter (LoRA) finetuning [ 28] is a method that reduces memory\\nrequirements by using a small set of trainable parameters, often termed adapters, while not updating\\nthe full model parameters which remain fixed. Gradients during stochastic gradient descent are\\npassed through the fixed pretrained model weights to the adapter, which is updated to optimize the\\nloss function. LoRA augments a linear projection through an additional factorized projection. Given\\na projection XW =YwithX∈Rb×h,W∈Rh×oLoRA computes:\\nY=XW +sXL 1L2, (3)\\nwhereL1∈Rh×randL2∈Rr×o, and sis a scalar.\\nMemory Requirement of Parameter-Efficient Finetuning One important point of discussion is\\nthe memory requirement of LoRA during training both in terms of the number and size of adapters\\nused. Since the memory footprint of LoRA is so minimal, we can use more adapters to improve\\nperformance without significantly increasing the total memory used. While LoRA was designed as a\\n3Parameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning\\ncomes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA\\nmodel trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used\\n0.2% of the original model weights[ 28,37], the LoRA input gradients have a memory footprint\\nof 567 MB while the LoRA parameters take up only 26 MB. With gradient checkpointing [ 9], the\\ninput gradients reduce to an average of 18 MB per sequence making them more memory intensive\\nthan all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of\\nmemory. This highlights that gradient checkpointing is important but also that aggressively reducing\\nthe amount of LoRA parameter yields only minor memory benefits. This means we can use more\\nadapters without significantly increasing the overall training memory footprint (see Appendix G\\nfor a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision\\nperformance.\\n3 QL ORA Finetuning\\nQLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat\\n(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to\\nprevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\\ntraditionally made finetuning on a single machine difficult for large models.\\nQLORAhas one low-precision storage data type, in our case usually 4-bit, and one computation data\\ntype that is usually BFloat16. In practice, this means whenever a QLORAweight tensor is used, we\\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\\nWe now discuss the components of QL ORA followed by a formal definition of QL ORA.\\n4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization\\n[15] which is an information-theoretically optimal data type that ensures each quantization bin has an\\nequal number of values assigned from the input tensor. Quantile quantization works by estimating\\nthe quantile of the input tensor through the empirical cumulative distribution function.\\nThe main limitation of quantile quantization is that the process of quantile estimation is expensive.\\nTherefore fast quantile approximation algorithms, such as SRAM quantiles [ 15], are used to estimate\\nthem. Due to the approximate nature of these quantile estimation algorithms, the data type has large\\nquantization errors for outliers, which are often the most important values.\\nExpensive quantile estimates and approximation errors can be avoided when input tensors come from\\na distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles\\nmaking exact quantile estimation computationally feasible.\\nSince pretrained neural network weights usually have a zero-centered normal distribution with\\nstandard deviation σ(see Appendix F), we can transform all weights to a single fixed distribution by\\nscaling σsuch that the distribution fits exactly into the range of our data type. For our data type, we\\nset the arbitrary range [−1,1]. As such, both the quantiles for the data type and the neural network\\nweights need to be normalized into this range.\\nThe information theoretically optimal data type for zero-mean normal distributions with arbitrary\\nstandard deviations σin the range [−1,1]is computed as follows: (1) estimate the 2k+ 1quantiles\\nof a theoretical N(0,1)distribution to obtain a k-bit quantile quantization data type for normal distri-\\nbutions, (2) take this data type and normalize its values into the [−1,1]range, (3) quantize an input\\nweight tensor by normalizing it into the [−1,1]range through absolute maximum rescaling.\\nOnce the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to\\nrescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data\\ntype. More formally, we estimate the 2kvalues qiof the data type as follows:\\nqi=1\\n2\\x12\\nQX\\x12i\\n2k+ 1\\x13\\n+QX\\x12i+ 1\\n2k+ 1\\x13\\x13\\n, (4)\\nwhere QX(·)is the quantile function of the standard normal distribution N(0,1). A problem for\\na symmetric k-bit quantization is that this approach does not have an exact representation of zero,\\nwhich is an important property to quantize padding and other zero-valued elements with no error. To\\n4ensure a discrete zeropoint of 0and to use all 2kbits for a k-bit datatype, we create an asymmetric\\ndata type by estimating the quantiles qiof two ranges qi:2k−1for the negative part and 2k−1+ 1for\\nthe positive part and then we unify these sets of qiand remove one of the two zeros that occurs in both\\nsets. We term the resulting data type that has equal expected number of values in each quantization bin\\nk-bit NormalFloat (NFk), since the data type is information-theoretically optimal for zero-centered\\nnormally distributed data. The exact values of this data type can be found in Appendix E.\\nDouble Quantization We introduce Double Quantization (DQ), the process of quantizing the\\nquantization constants for additional memory savings. While a small blocksize is required for precise\\n4-bit quantization [ 13], it also has a considerable memory overhead. For example, using 32-bit\\nconstants and a blocksize of 64 for W, quantization constants add 32/64 = 0 .5bits per parameter on\\naverage. Double Quantization helps reduce the memory footprint of quantization constants.\\nMore specifically, Double Quantization treats quantization constants cFP32\\n2of the first quantization\\nas inputs to a second quantization. This second step yields the quantized quantization constants\\ncFP8\\n2and the second level of quantization constants cFP32\\n1. We use 8-bit Floats with a blocksize of\\n256 for the second quantization as no performance degradation is observed for 8-bit quantization,\\nin line with results from Dettmers and Zettlemoyer [13]. Since the cFP32\\n2are positive, we subtract\\nthe mean from c2before quantization to center the values around zero and make use of symmetric\\nquantization. On average, for a blocksize of 64, this quantization reduces the memory footprint per\\nparameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64·256) = 0 .127bits, a reduction of 0.373 bits\\nper parameter.\\nPaged Optimizers use the NVIDIA unified memory3feature wich does automatic page-to-page\\ntransfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\\noccasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\\nand the disk. We use this feature to allocate paged memory for the optimizer states which are then\\nautomatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\\nmemory when the memory is needed in the optimizer update step.\\nQL ORA.Using the components described above, we define QLORAfor a single linear layer in\\nthe quantized base model with a single LoRA adapter as follows:\\nYBF16=XBF16doubleDequant (cFP32\\n1, ck-bit\\n2,WNF4) +XBF16LBF16\\n1LBF16\\n2, (5)\\nwhere doubleDequant (·)is defined as:\\ndoubleDequant (cFP32\\n1, ck-bit\\n2,Wk-bit) =dequant (dequant (cFP32\\n1, ck-bit\\n2),W4bit) =WBF16,(6)\\nWe use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\\nand a blocksize of 256 for c2to conserve memory.\\nFor parameter updates only the gradient with respect to the error for the adapters weights∂E\\n∂Liare\\nneeded, and not for 4-bit weights∂E\\n∂W. However, the calculation of∂E\\n∂Lientails the calculation of∂X\\n∂W\\nwhich proceeds via equation (5) with dequantization from storage WNF4to computation data type\\nWBF16to calculate the derivative∂X\\n∂Win BFloat16 precision.\\nTo summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\\ndata type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\\nto perform the forward and backward pass, but we only compute weight gradients for the LoRA\\nparameters which use 16-bit BrainFloat.\\n4 QLoRA vs. Standard Finetuning\\nWe have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model\\nfinetuning. Furthermore, we want to analyze the components of QLoRA including the impact of\\nNormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed\\nat answering these questions.\\n3https://docs.nvidia.com/cuda/cuda-c-programming-guide\\n5Experimental setup. We consider three architectures (encoder, encoder-decoder, and decoder only)\\nand compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our\\nevaluations include GLUE [ 58] with RoBERTa-large [ 38], Super-NaturalInstructions (TKInstruct)\\n[61] with T5 [ 49], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [ 39] and Alpaca\\n[55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of\\nDettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity\\nacross different models (OPT [ 72], LLaMA [ 57], BLOOM [ 52], Pythia [ 7]) for model sizes 125m -\\n13B. We provide more details in the results section for each particular setup to make the results more\\nreadable. Full details in Appendix A.\\nQLoRA-AllQLoRA-FFN\\nQLoRA-AttentionAlpaca (ours)\\nStanford-Alpaca\\nModel6061626364RougeL\\nbits\\n4\\n16\\nFigure 2: RougeL for LLaMA 7B models on the\\nAlpaca dataset. Each point represents a run with a\\ndifferent random seed. We improve on the Stanford\\nAlpaca fully finetuned default hyperparameters to\\nconstruct a strong 16-bit baseline for comparisons.\\nUsing LoRA on all transformer layers is critical to\\nmatch 16-bit performance.While paged optimizers are critical to do 33B/65B\\nQLORAtuning on a single 24/48GB GPU, we do\\nnot provide hard measurements for Paged Optimiz-\\ners since the paging only occurs when processing\\nmini-batches with long sequence lengths, which is\\nrare. We do, however, perform an analysis of the\\nruntime of paged optimizers for 65B models on\\n48GB GPUs and find that with a batch size of 16,\\npaged optimizers provide the same training speed\\nas regular optimizers. Future work should measure\\nand characterize under what circumstances slow-\\ndowns occur from the paging process.\\nDefault LoRA hyperparameters do not match 16-\\nbit performance When using the standard prac-\\ntice of applying LoRA to query and value attention\\nprojection matrices [ 28], we are not able to replicate\\nfull finetuning performance for large base models.\\nAs shown in Figure 2 for LLaMA 7B finetuning on\\nAlpaca, we find that the most critical LoRA hyper-\\nparameter is how many LoRA adapters are used in\\ntotal and that LoRA on all linear transformer block\\nlayers are required to match full finetuning perfor-\\nmance. Other LoRA hyperparameters, such as the\\nprojection dimension r, do not affect performance (see Appendix A).\\n1010\\n1011\\nT otal model bits\\n0.60\\n0.61\\n0.62\\n0.63\\n0.64\\n0.65\\n0.66\\n0.67Mean zeroshot accuracy\\n4-bit LLaMA\\nFloat\\nNFloat\\nNFloat + DQData type\\nFigure 3: Mean zero-shot accuracy over Wino-\\ngrande, HellaSwag, PiQA, Arc-Easy, and Arc-\\nChallenge using LLaMA models with different 4-bit\\ndata types. The NormalFloat data type significantly\\nimproves the bit-for-bit accuracy gains compared\\nto regular 4-bit Floats. While Double Quantization\\n(DQ) only leads to minor gains, it allows for a more\\nfine-grained control over the memory footprint to fit\\nmodels of certain size (33B/65B) into certain GPUs\\n(24/48GB).Similarly, we find that default hyperparameters for\\nfully finetuned baselines are undertuned. We do a\\nhyperparameter search over learning rates 1e-6 to\\n5e-5 and batch sizes 8 to 128 to find robust baselines.\\nResults for 7B LLaMA finetuning on Alpaca are\\nshown in Figure 2.\\n4-bit NormalFloat yields better performance\\nthan 4-bit Floating Point While the 4-bit\\nNormalFloat (NF4) data type is information-\\ntheoretically optimal, it still needs to be determined\\nif this property translates to empirical advantages.\\nWe follow the setup from Dettmers and Zettlemoyer\\n[13] where quantized LLMs (OPT [ 72], BLOOM\\n[52], Pythia [ 7], LLaMA) of different sizes (125M\\nto 65B) with different data types are evaluated on\\nlanguage modeling and a set of zero-shot tasks. In\\nFigure 3 and Table 2 we see that NF4 improves per-\\nformance significantly over FP4 and Int4 and that\\ndouble quantization reduces the memory footprint\\nwithout degrading performance.\\nk-bit QL ORAmatches 16-bit full finetuning and\\n16-bit LoRA performance Recent findings have\\nestablished that 4-bit quantization for inference is\\n6'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_of_tokens(text):\n",
    "    return len(word_tokenize(text))\n",
    "all_tokens=0\n",
    "first_q_pages_text=''\n",
    "for page in range(0,len(pages)//4):\n",
    "    first_q_pages_text+=pages[page]\n",
    "    all_tokens+=num_of_tokens(pages[page])\n",
    "first_q_pages_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
