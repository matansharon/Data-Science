{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import PyPDF2\n",
    "def summeriztion(file_path:str=\"\",start_page:int=0,end_page:int=10):\n",
    "    \n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    file=PyPDF2.PdfReader(file_path)\n",
    "    text=\"\"\n",
    "    for pos in range(start_page,end_page):\n",
    "        if len(word_tokenize(text))<4000:\n",
    "            text+=file.pages[pos].extract_text()\n",
    "    print(len(word_tokenize(text)))\n",
    "        \n",
    "    \n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"summarize the following \"+text,\n",
    "            }\n",
    "        ],\n",
    "        model=\"mixtral-8x7b-32768\"\n",
    "        # model=\"llama3-70b-8192\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simple_chat(text):\n",
    "#     chat_completion = client.chat.completions.create(\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\":f\"answer the following question: {text}\",\n",
    "#             }\n",
    "#         ],\n",
    "#         model=\"mixtral-8x7b-32768\"\n",
    "#         # model=\"llama3-70b-8192\",\n",
    "#     )\n",
    "    \n",
    "#     return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=simple_chat(\"write me a python function that recive a device name and return and output from the FDA API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_device_info(device_name):\n",
    "    # The base URL for the OpenFDA device database\n",
    "    base_url = \"https://api.fda.gov/device/classifction.json\"\n",
    "\n",
    "    # The parameters for the API request\n",
    "    params = {\n",
    "        \"search\": f\"device.device_name:{device_name}\",\n",
    "        \"limit\": 10\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Return the JSON response\n",
    "        return response.json()\n",
    "    else:\n",
    "        # If the request was not successful, return an error message\n",
    "        return {\"error\": \"Failed to retrieve data from FDA API\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_device_info(\"Stopcock i.v set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=summeriztion('/Users/matansharon/python/Data_science/papers/SciPy and NumPy pdf - EBook Free Download ( PDFDrive ).pdf',30,50)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "def summerize_and_add_python_example(text):\n",
    "    client = Groq(\n",
    "        api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    )\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"this is a video transcription \"+text+\"\\n\\n\\n summerize it and add a python script that will demonstrate the concept\",\n",
    "            }\n",
    "        ],\n",
    "        # model=\"mixtral-8x7b-32768\"\n",
    "        model=\"llama3-70b-8192\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=\"\"\n",
    "with open(\"/Users/matansharon/python/Data_science/text files/transcript.txt\",\"r\") as file:\n",
    "    text=file.read()\n",
    "    # print(text)\n",
    "    # print(len(word_tokenize(text)))\n",
    "    res=summerize_and_add_python_example(text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import PyPDF2\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "audio_path=\"/Users/matansharon/python/Data_science/some tests/Building the world first AI sports narrator (Hebrew).mp3\"\n",
    "audio_file=open(audio_path,\"rb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da699e8315674f51a9dbc61fe1dfee69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not '_io.BufferedReader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# audio_resample based on entry being part of an existing dataset.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Alternatively, this can be loaded from an audio file.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# audio_resample = librosa.resample(entry['audio']['array'], orig_sr=entry['audio']['sampling_rate'], target_sr=SAMPLING_RATE)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m audio_resmaple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/matansharon/python/Data_science/some tests/Building the world first AI sports narrator (Hebrew).mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m input_features \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_resmaple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLING_RATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_features\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cuda:\n\u001b[1;32m     22\u001b[0m   input_features \u001b[38;5;241m=\u001b[39m input_features\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/python/Data_science/.venv/lib/python3.10/site-packages/transformers/models/whisper/processing_whisper.py:70\u001b[0m, in \u001b[0;36mWhisperProcessor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either an `audio` or `text` input to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/python/Data_science/.venv/lib/python3.10/site-packages/transformers/models/whisper/feature_extraction_whisper.py:216\u001b[0m, in \u001b[0;36mWhisperFeatureExtractor.__call__\u001b[0;34m(self, raw_speech, truncation, pad_to_multiple_of, return_tensors, return_attention_mask, padding, max_length, sampling_rate, do_normalize, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray([speech], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mT \u001b[38;5;28;01mfor\u001b[39;00m speech \u001b[38;5;129;01min\u001b[39;00m raw_speech]\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_speech, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 216\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_speech\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_speech, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m raw_speech\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mfloat64):\n\u001b[1;32m    218\u001b[0m     raw_speech \u001b[38;5;241m=\u001b[39m raw_speech\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not '_io.BufferedReader'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "has_cuda = torch.cuda.is_available()\n",
    "model_path = 'ivrit-ai/whisper-large-v2-tuned'\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "if has_cuda:\n",
    "    model.to('cuda:0')\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "\n",
    "# audio_resample based on entry being part of an existing dataset.\n",
    "# Alternatively, this can be loaded from an audio file.\n",
    "audio_resample = librosa.resample(entry['audio']['array'], orig_sr=entry['audio']['sampling_rate'], target_sr=SAMPLING_RATE)\n",
    "audio_resmaple=open(\"/Users/matansharon/python/Data_science/some tests/Building the world first AI sports narrator (Hebrew).mp3\",\"rb\")\n",
    "\n",
    "input_features = processor(audio_resmaple, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\").input_features\n",
    "if has_cuda:\n",
    "  input_features = input_features.to('cuda:0')\n",
    "\n",
    "predicted_ids = model.generate(input_features, language='he', num_beams=5)\n",
    "transcript = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "# print(f'Transcript: {transcription[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
