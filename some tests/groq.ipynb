{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A token is a unit of text, such as a word, character, or subword, depending on the context and the task at hand. Calculating the number of tokens in a given text involves breaking down the text into its constituent parts. Here are some common ways to calculate the number of tokens:\n",
      "\n",
      "**Method 1: Word-level tokenization**\n",
      "\n",
      "Split the text into individual words, separated by spaces, punctuation, or special characters. This is the most common approach.\n",
      "\n",
      "Example: \"Hello, how are you?\" → [\"Hello,\", \"how\", \"are\", \"you?\"]\n",
      "\n",
      "Tokens: 4\n",
      "\n",
      "**Method 2: Character-level tokenization**\n",
      "\n",
      "Split the text into individual characters.\n",
      "\n",
      "Example: \"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]\n",
      "\n",
      "Tokens: 5\n",
      "\n",
      "**Method 3: Subword-level tokenization**\n",
      "\n",
      "Split the text into subwords, which are smaller units of text, such as word pieces or morphemes.\n",
      "\n",
      "Example: \"unbreakable\" → [\"un-\", \"break-\", \"able\"]\n",
      "\n",
      "Tokens: 3\n",
      "\n",
      "**Method 4: Tokenization using NLTK (Natural Language Toolkit)**\n",
      "\n",
      "Use a library like NLTK, which provides a tokenization function that splits the text into tokens based on word boundaries.\n",
      "\n",
      "Example (Python code):\n",
      "```python\n",
      "import nltk\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "text = \"Hello, how are you?\"\n",
      "tokens = word_tokenize(text)\n",
      "print(len(tokens))  # Output: 4\n",
      "```\n",
      "**Method 5: Tokenization using spaCy**\n",
      "\n",
      "Use a library like spaCy, which provides a tokenization function that splits the text into tokens based on word boundaries.\n",
      "\n",
      "Example (Python code):\n",
      "```python\n",
      "import spacy\n",
      "\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "text = \"Hello, how are you?\"\n",
      "doc = nlp(text)\n",
      "print(len(doc))  # Output: 4\n",
      "```\n",
      "The choice of tokenization method depends on the specific task, the type of text data, and the requirements of your project.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"how to calculate the number of tokens of given text?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Direction constants\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "LEFT = 4\n",
    "\n",
    "class SnakeGame:\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "        self.width, self.height = 800, 600\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption(\"Snake Game\")\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.direction = RIGHT\n",
    "        self.snake = [(200, 200), (220, 200), (240, 200)]\n",
    "        self.apple = self.get_random_apple()\n",
    "        self.score = 0\n",
    "\n",
    "    def get_random_apple(self):\n",
    "        x = random.randint(0, (self.width - 10) // 10) * 10\n",
    "        y = random.randint(0, (self.height - 10) // 10) * 10\n",
    "        return (x, y)\n",
    "\n",
    "    def play(self):\n",
    "        while True:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_UP and self.direction != DOWN:\n",
    "                        self.direction = UP\n",
    "                    elif event.key == pygame.K_DOWN and self.direction != UP:\n",
    "                        self.direction = DOWN\n",
    "                    elif event.key == pygame.K_LEFT and self.direction != RIGHT:\n",
    "                        self.direction = LEFT\n",
    "                    elif event.key == pygame.K_RIGHT and self.direction != LEFT:\n",
    "                        self.direction = RIGHT\n",
    "\n",
    "            self.move_snake()\n",
    "            self.check_collision()\n",
    "            self.draw_game()\n",
    "\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(10)\n",
    "\n",
    "    def move_snake(self):\n",
    "        head = self.snake[0]\n",
    "        if self.direction == UP:\n",
    "            new_head = (head[0], head[1] - 10)\n",
    "        elif self.direction == RIGHT:\n",
    "            new_head = (head[0] + 10, head[1])\n",
    "        elif self.direction == DOWN:\n",
    "            new_head = (head[0], head[1] + 10)\n",
    "        elif self.direction == LEFT:\n",
    "            new_head = (head[0] - 10, head[1])\n",
    "\n",
    "        self.snake.insert(0, new_head)\n",
    "\n",
    "        if self.snake[0] == self.apple:\n",
    "            self.score += 1\n",
    "            self.apple = self.get_random_apple()\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "\n",
    "    def check_collision(self):\n",
    "        if (self.snake[0][0] < 0 or self.snake[0][0] > self.width - 10 or\n",
    "            self.snake[0][1] < 0 or self.snake[0][1] > self.height - 10):\n",
    "            self.game_over()\n",
    "        for i in range(1, len(self.snake)):\n",
    "            if self.snake[0] == self.snake[i]:\n",
    "                self.game_over()\n",
    "\n",
    "    def game_over(self):\n",
    "        print(\"Game Over! Your score is\", self.score)\n",
    "        self.reset()\n",
    "\n",
    "    def draw_game(self):\n",
    "        self.screen.fill((255, 255, 255))\n",
    "        for pos in self.snake:\n",
    "            pygame.draw.rect(self.screen, (0, 255, 0), pygame.Rect(pos[0], pos[1], 10, 10))\n",
    "        pygame.draw.rect(self.screen, (255, 0, 0), pygame.Rect(self.apple[0], self.apple[1], 10, 10))\n",
    "        pygame.display.update()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    game = SnakeGame()\n",
    "    game.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    file=PyPDF2.PdfReader(pdf_path)\n",
    "    return file\n",
    "\n",
    "file=extract_text_from_pdf('/Users/matansharon/python/Data_science/papers/attention is all you need.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.pages[10].extract_text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
