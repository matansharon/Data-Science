{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " take over the world, but first it needs to get better at writing poetry.\n",
      "\n",
      "That's according to a recent project from researchers at the University of California, Berkeley, who used machine learning algorithms to generate haiku poems about winter in Japanese style. The team's findings were published in the journal PLOS ONE.\n",
      "\n",
      "The goal of this research was to showcase the advancement in machine learning techniques that can be applied to the creation of art and literature.\n",
      "\n",
      "\"Poetry is a very interesting application for natural language processing,\" said study author Mirella Lapata, a professor at UC Berkeley's School of Information, in a statement. \"It's not just about understanding meaning or syntax. Poems have rhythm, they have metaphors, and they have emotions.\"\n",
      "\n",
      "The researchers trained their AI on 17 centuries' worth of haiku poems from Japanese masters and collected information about the words, syllables, grammar, and structure of those works to generate new ones. They tested their algorithm with 43 lines written by different poets in various styles, including some that contained metaphors or unusual phrasing.\n",
      "\n",
      "The results were impressive: The AI produced 21 lines of winter\n"
     ]
    }
   ],
   "source": [
    "llm = CTransformers(model='/teamspace/studios/this_studio/mistral-7b-instruct-v0.2.Q4_K_M.gguf', model_type='gpt2')\n",
    "\n",
    "print(llm.invoke('AI is going to'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s> [INST] what is qlora? [/INST]\n",
      "\n",
      "QLora is not a widely known term or concept in general. It might refer to a specific software application, company, or research project, but without more context, it's difficult to provide an accurate definition. Here are a few possibilities based on limited information:\n",
      "\n",
      "1. QLora is a programming language: QLora is a new, experimental programming language. However, I couldn't find any credible sources that confirm this.\n",
      "2. QLora is a marketing analytics tool: QLora is a marketing analytics and reporting tool developed by Quarkbiz. It helps businesses to analyze their marketing data, create reports, and visualize insights.\n",
      "3. QLora is a research project: QLora is a research project focusing on natural language processing, machine learning, and artificial intelligence, but I couldn't find any official website or documentation related to it.\n",
      "\n",
      "If you have more information about QLora or if you meant a different term, please provide more context, and I'll be happy to help you out.</s>\n",
      "*** Pipeline:\n",
      "<s>[INST] what is qlora? [/INST]\n",
      "I'm sorry for any confusion, but I couldn't find specific information about a term called \"QLora.\" It's possible that it could be a typo or an unfamiliar acronym or brand name. Could you please provide some additional context or details about what you mean by \"QLora\"? It might help to double-check the spelling or do some research to see if it refers to a particular technology, company, or concept in a specific industry. If you meant to ask about something else, feel free to ask and I'll do my best to help.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"/teamspace/studios/this_studio/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"what is qlora?\"\n",
    "prompt_template=f'''<s>[INST] {prompt} [/INST]\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
