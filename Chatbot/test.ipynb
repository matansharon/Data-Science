{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 3040, which is longer than the specified 1000\n",
      "Created a chunk of size 4804, which is longer than the specified 1000\n",
      "Created a chunk of size 2749, which is longer than the specified 1000\n",
      "Created a chunk of size 4375, which is longer than the specified 1000\n",
      "Created a chunk of size 4086, which is longer than the specified 1000\n",
      "Created a chunk of size 4103, which is longer than the specified 1000\n",
      "Created a chunk of size 3846, which is longer than the specified 1000\n",
      "Created a chunk of size 4049, which is longer than the specified 1000\n",
      "Created a chunk of size 5174, which is longer than the specified 1000\n",
      "Created a chunk of size 4053, which is longer than the specified 1000\n",
      "Created a chunk of size 3185, which is longer than the specified 1000\n",
      "Created a chunk of size 2725, which is longer than the specified 1000\n",
      "Created a chunk of size 2377, which is longer than the specified 1000\n",
      "Created a chunk of size 4521, which is longer than the specified 1000\n",
      "Created a chunk of size 3966, which is longer than the specified 1000\n",
      "Created a chunk of size 3558, which is longer than the specified 1000\n",
      "Created a chunk of size 3465, which is longer than the specified 1000\n",
      "Created a chunk of size 3594, which is longer than the specified 1000\n",
      "Created a chunk of size 3403, which is longer than the specified 1000\n",
      "Created a chunk of size 3779, which is longer than the specified 1000\n",
      "Created a chunk of size 3110, which is longer than the specified 1000\n",
      "Created a chunk of size 3504, which is longer than the specified 1000\n",
      "Created a chunk of size 3227, which is longer than the specified 1000\n",
      "Created a chunk of size 1409, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"/Users/matansharon/python/Data_science/Chatbot/data/qlora.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "llm=ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "template = \"\"\"\n",
    "you are a helpful assitant,\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. Qlora is a decentralized platform for knowledge sharing and community building.\\n2. It allows users to create, share, and discover content on a wide range of topics.\\n3. Users can earn rewards for contributing valuable content and engaging with the community.\\n4. Qlora uses blockchain technology to ensure transparency, security, and fairness in the platform.\\n5. Overall, qlora aims to empower individuals to learn, connect, and collaborate in a decentralized and incentivized manner.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QLORA is a finetuning method that improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. It aims to reduce memory requirements while maintaining performance in tasks such as language modeling and instruction following. The experiments conducted with QLORA show that it can match the performance of full 16-bit finetuning and 16-bit LoRA finetuning on academic benchmarks. Additionally, QLORA with NormalFloat (NF4) data type has been shown to be more effective than with Float4 (FP4) in terms of quantization precision. Overall, QLORA is a method that allows for efficient parameter-efficient finetuning with reduced memory footprint.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='what is qlora?'\n",
    "db_ans = db.similarity_search(query)\n",
    "llm_chain.run(f\"base on the folowing docs {db_ans} what is qlora?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
