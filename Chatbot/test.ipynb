{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader,DirectoryLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "def load_and_split_docs():\n",
    "\n",
    "# load the document and split it into chunks\n",
    "    list_of_name=os.listdir('/Users/matansharon/python/Data_science/Chatbot/data')\n",
    "    for doc in docs:\n",
    "        with open(f'/Users/matansharon/python/Data_science/Chatbot/data/{doc}', 'r') as file:\n",
    "            docs=file.read()\n",
    "            loader = TextLoader(docs)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# split it into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "def run__openSource_embedding(docs):\n",
    "# create the open-source embedding function\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    docs=docs\n",
    "# load it into Chroma\n",
    "    db = Chroma.from_documents(docs, embedding_function)\n",
    "    return db\n",
    "def create_llm_chain(llm_name:str='gpt-3.5-turbo',temperture: float=0):\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    template = \"\"\"\n",
    "        you are a helpful assitant,\n",
    "        Question: {question}\n",
    "\n",
    "        Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    return llm_chain\n",
    "def run():\n",
    "    docs=load_and_split_docs()\n",
    "    db=run__openSource_embedding(docs)\n",
    "    llm_chain=create_llm_chain('gpt-3.5-turbo')\n",
    "    return llm_chain,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading ['harry-potter-book-collection-1-4.txt', 'qlora.txt', 'lora.txt', '2021.acl-long.353.txt', 'instruct embedding.txt', 'SciPy and NumPy pdf - EBook Free Download ( PDFDrive ).txt', 'Attention Is All You Need.txt', 'Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.txt', 'Approaching (Almost) Any Machine Learning Problem.txt']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/python/Data_science/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/text.py:41\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     42\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm,db\u001b[38;5;241m=\u001b[39m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m():\n\u001b[0;32m---> 41\u001b[0m     docs\u001b[38;5;241m=\u001b[39m\u001b[43mload_and_split_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     db\u001b[38;5;241m=\u001b[39mrun__openSource_embedding(docs)\n\u001b[1;32m     43\u001b[0m     llm_chain\u001b[38;5;241m=\u001b[39mcreate_llm_chain(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mload_and_split_docs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     docs\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/matansharon/python/Data_science/Chatbot/data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m     loader \u001b[38;5;241m=\u001b[39m TextLoader(docs)\n\u001b[0;32m---> 16\u001b[0m     documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m documents\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# split it into chunks\u001b[39;00m\n",
      "File \u001b[0;32m~/python/Data_science/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/text.py:57\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     59\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path}\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39mmetadata)]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error loading ['harry-potter-book-collection-1-4.txt', 'qlora.txt', 'lora.txt', '2021.acl-long.353.txt', 'instruct embedding.txt', 'SciPy and NumPy pdf - EBook Free Download ( PDFDrive ).txt', 'Attention Is All You Need.txt', 'Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.txt', 'Approaching (Almost) Any Machine Learning Problem.txt']"
     ]
    }
   ],
   "source": [
    "llm,db=run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QLORA is a finetuning method that improves over LoRA by quantizing the transformer model to 4-bit precision and using paged optimizers to handle memory spikes. It aims to reduce memory requirements while maintaining performance in tasks such as language modeling and instruction following. The experiments conducted with QLORA show that it can match the performance of full 16-bit finetuning and 16-bit LoRA finetuning on academic benchmarks. Additionally, QLORA with NormalFloat (NF4) data type has been shown to be more effective than with Float4 (FP4) in terms of quantization precision. Overall, QLORA is a method that allows for efficient parameter-efficient finetuning with reduced memory footprint.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='what is qlora?'\n",
    "db_ans = db.similarity_search(query)\n",
    "llm_chain.run(f\"base on the folowing docs {db_ans} what is qlora?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_docs():\n",
    "\n",
    "# load the document and split it into chunks\n",
    "    loader=DirectoryLoader('/Users/matansharon/python/Data_science/Chatbot/data')\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter()\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "docs=load_and_split_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3876\n",
      "3987\n",
      "3732\n",
      "3908\n",
      "3965\n",
      "3931\n",
      "3980\n",
      "3820\n",
      "3859\n",
      "3789\n",
      "3805\n",
      "3709\n",
      "3908\n",
      "3949\n",
      "3988\n",
      "3861\n",
      "3839\n",
      "4000\n",
      "3964\n",
      "3576\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,20):\n",
    "    print(len(docs[i].page_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
