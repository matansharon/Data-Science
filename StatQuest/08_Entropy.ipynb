{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy in Data Science - Key Concepts\n",
    "\n",
    "### Introduction to Surprise\n",
    "- Entropy is linked to the concept of 'surprise'.\n",
    "- In a system (e.g., chickens of different colors in different areas), the probability of an event is inversely related to the surprise it generates.\n",
    "- Lower probability events yield higher surprise and vice versa.\n",
    "\n",
    "### Equation for Surprise\n",
    "- Surprise is calculated using the logarithm of the inverse of the probability.\n",
    "- Formula: `Surprise = -log2(probability)`\n",
    "- This formula ensures that an event with a certain probability of occurring gives a quantifiable measure of surprise.\n",
    "\n",
    "### Calculating Surprise for a Series of Events\n",
    "- Total surprise for a sequence of events (like coin tosses) is the sum of individual surprises.\n",
    "- For a biased coin, the surprise differs for heads and tails based on their probabilities.\n",
    "\n",
    "### Entropy Defined for a Coin\n",
    "- Entropy is the expected value of surprise per event.\n",
    "- In statistics: `Entropy = -sum(probability * log2(probability))`\n",
    "- It represents the average surprise (or uncertainty) we expect from the outcome of an event.\n",
    "\n",
    "### Entropy in Action\n",
    "- Entropy is used to quantify the similarity or difference in distributions (like the distribution of chicken colors).\n",
    "- Higher entropy indicates a more uniform distribution (equal numbers of different outcomes).\n",
    "- Lower entropy suggests a skewed distribution (one outcome is far more likely than others).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_entropy(probabilities):\n",
    "    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "# Example: A biased coin with a 90% chance of heads and 10% chance of tails\n",
    "probabilities = [0.9, 0.1]\n",
    "entropy = calculate_entropy(probabilities)\n",
    "\n",
    "print(f\"Entropy of the biased coin: {entropy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
